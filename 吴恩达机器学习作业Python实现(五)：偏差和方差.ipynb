{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-1-8a113401a82c>, line 55)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-8a113401a82c>\"\u001b[0;36m, line \u001b[0;32m55\u001b[0m\n\u001b[0;31m    def plot_learning_curve(X_train,y_train,X_val,y_val,lamda):\u001b[0m\n\u001b[0m                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "data = loadmat('ex5data1.mat')\n",
    "\n",
    "dict_keys(['__header__', '__version__', '__globals__', 'X', 'y', 'Xtest', 'ytest', 'Xval', 'yval'])\n",
    "# 训练集\n",
    "X_train,y_train=data['X'],data['y']\n",
    "#(12,1) (12,1)\n",
    "X_val,y_val=data['Xval'],data['yval']\n",
    "\n",
    "X_test,y_test=data['Xtest'],data['ytest']\n",
    "\n",
    "#输入变量插入偏置\n",
    "X_train=np.insert(X_train,0,1,axis=1)\n",
    "X_val=np.insert(X_val,0,1,axis=1)\n",
    "X_test=np.insert(X_test,0,1,axis=1)\n",
    "\n",
    "#linear regression \n",
    "#线性回归，代价函数，带正则化\n",
    "def reg_cost(theta,X,y,lamda):\n",
    "    cost=np.sum(np.power((X@theta-y.flatten()),2))\n",
    "    reg=theta[1:]@theta[1:]*lamda\n",
    "    return (cost+reg)/(2*len(X))\n",
    "#参数初始化\n",
    "theta=np.ones(X_train.shape[1])\n",
    "lamda=1\n",
    "loss=re_cost(theta,X_train,y_trian,lamda)  #输出代价loss\n",
    "\n",
    "\n",
    "#梯度函数，带正则化\n",
    "#输出代价函数的梯度\n",
    "def reg_gradient(theta,X,y,lamda):\n",
    "    grad=(X@theta-y.flatten())@X\n",
    "    reg=lamda*theta\n",
    "    reg[0]=0#第一项不参加正则化运算\n",
    "    return (grad+reg)/(len(X))\n",
    "reg_gradient(theta,X_train,y_train,lamda)\n",
    "\n",
    "\n",
    "#利用给出的代价函数和梯度，更新theta,使得代价函数输出最小\n",
    "def train_model(X,y,lamda):\n",
    "    theta=np.ones(X.shape[1])\n",
    "    res=minimize(fun=reg_cost,x0=theta,args=(X,y,lamda),method='TNC',jac=reg_gradient)\n",
    "    return res.x  #训练，返回使得代价函数最小时的，theta值\n",
    "theta_final = train_model(X_train,y_train,lamda=1)\n",
    "\n",
    "plot_data()\n",
    "plt.plot(X_train[:,1],X_train@theta_final,c='r')\n",
    "plt.show()\n",
    "\n",
    "#学习曲线\n",
    "def plot_learning_curve(X_train,y_train,X_val,y_val,lamda):\n",
    "    \n",
    "    x = range(1,len(X_train)+1)\n",
    "    training_cost = []\t\t# 训练集的误差\n",
    "    cv_cost = []\t\t\t# 验证集的误差\n",
    "    \n",
    "    for i in x:        \n",
    "        res = train_model(X_train[:i,:],y_train[:i,:],lamda) #训练集取样本个数 i\n",
    "        \n",
    "        #需要注意的是，当计算训练集、交叉验证集和测试集误差时，不计算正则项，所以令 λ=0\n",
    "        training_cost_i = reg_cost(res,X_train[:i,:],y_train[:i,:],lamda = 0)\n",
    "        cv_cost_i = reg_cost(res,X_val,y_val,lamda = 0) #验证集取自己全部样本\n",
    "        training_cost.append(training_cost_i)   \n",
    "        cv_cost.append(cv_cost_i)\t\n",
    "        \t\t\t\n",
    "    plt.plot(x,cv_cost,label = 'cv cost')\n",
    "    plt.plot(x,training_cost,label = 'training cost')\n",
    "    plt.legend()\n",
    "    plt.xlabel('number of training examples')\n",
    "    plt.ylabel('error')\n",
    "    plt.show()\n",
    "    \n",
    "plot_learning_curve(X_train,y_train,X_val,y_val,lamda)\n",
    "\n",
    "    #由图可知，当Jtrain和Jcv都很大时候，属于欠拟合问题，此时增加训练数据是没有用的，可以引入更多的相关特征来降低偏差。\n",
    "#多项式回归\n",
    "#进行特征映射获取特征多项式\n",
    "def poly_feature(X,power):    \n",
    "    for i in range(2,power+1):\n",
    "        X = np.insert(X,X.shape[1],np.power(X[:,1],i),axis=1) # 在列尾插入\n",
    "    return X\n",
    "# 获得均值与方差\n",
    "def get_mean_std(x):\n",
    "    means = np.mean(x, axis=0) #按行取均值和方差\n",
    "    std = np.std(x, axis=0)  \n",
    "    return means,std\n",
    "    \n",
    "# 特征归一化\n",
    "def feature_normalize(X,means,stds):    \n",
    "    X[:,1:] = ( X[:,1:]  - means[1:]) / stds[1:]  # 第一列全为1，不用归一化\n",
    "    return X\n",
    "\n",
    "#获取特征多项式\n",
    "power = 6\n",
    "\n",
    "X_train_poly = poly_feature(X_train,power)# 获取特征多项式\n",
    "X_val_poly = poly_feature(X_val,power)\n",
    "X_test_poly = poly_feature(X_test,power)\n",
    "\n",
    "train_means,train_stds = get_means_stds(X_train_poly)# 均值和方差\n",
    "\n",
    "X_train_norm = feature_normalize(X_train_poly,train_means,train_stds)# 特征归一化\n",
    "X_val_norm = feature_normalize(X_val_poly,train_means,train_stds)\n",
    "X_test_norm = feature_normalize(X_test_poly,train_means,train_stds)\n",
    "\n",
    "theta_fit = train_model(X_train_norm,y_train,lamda=0)# 训练新特征，获得新的优化模型参数\n",
    "\n",
    "def plot_poly_fit():#绘制拟合曲线\n",
    "    plot_data()#绘制原始数据\n",
    "    \n",
    "    x = np.linspace(-60,60,100)\n",
    "    xx = x.reshape(100,1)\n",
    "    xx = np.insert(xx,0,1,axis=1)\n",
    "    xx = poly_feature(xx,power)\n",
    "    xx = feature_normalize(xx,train_means,train_stds)#获取横坐标的特征多项式\n",
    "    \n",
    "    plt.plot(x, xx @ theta_fit,'r--')# xx @ theta_fit即为曲线的y值\n",
    "\n",
    "plot_poly_fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
