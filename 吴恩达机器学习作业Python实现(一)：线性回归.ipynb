{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword can't be an expression (<ipython-input-4-9a654e8b7f52>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-9a654e8b7f52>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    data = pd.read_csv(path.header=None,names=['Population','Profit'])\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m keyword can't be an expression\n"
     ]
    }
   ],
   "source": [
    "单变量线性回归population-->profit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot.pyplot as plt\n",
    "path = 'ex1data1.txt'\n",
    "data = pd.read_csv(path,header=None,names=['Population','Profit'])\n",
    "#可视化数据\n",
    "data.plot(kind='scatter',x='Population',y='Profit',figsize=(8,5))\n",
    "#代价函数\n",
    "def computeCost(X,y,theta):\n",
    "    #X为nXm的矩阵，theta为1Xm\n",
    "    inner=np.power(((X*theta.T)-y),2)\n",
    "    return np.sum(inner)/(2*len(X))\n",
    "#我们在训练集中添加一列,X0\n",
    "data.insert(0,'Ones',1)\n",
    "#取最后一列为y,其余为X\n",
    "cols=data.shape[1]#列数\n",
    "X=data.iloc[:,0:cols-1]\n",
    "y=data.iloc[:,cols-1:]\n",
    "#这里我们使用的时matrix\n",
    "#对应元素相乘：matrix可以用np.multiply(X2,X1)，array直接X1*X2\n",
    "#点乘：matrix直接X1*X2，array可以 X1@X2 或 X1.dot(X2) 或 np.dot(X1, X2)\n",
    "X=np.matrix(X.values)\n",
    "y=np.matrix(y.values)\n",
    "theta=np.matrix([0,0])\n",
    "#theta为1X2\n",
    "#X.shape (97,2) theta.shape (1,2) y.shape(97,1)\n",
    "#计算初始代价函数的值\n",
    "J_theta=computeCost(X,y,theta)\n",
    "#批量梯度下降,更新theta,使得J_theta最小\n",
    "def gradientDescent(X,y,theta,alpha,epoch):\n",
    "    temp=np.matrix(np.zeros(theta.shape))#初始化theta,临时矩阵\n",
    "    parameters=int(theta.flatten().shape[1])\n",
    "    cost=np.zeros(epoch)\n",
    "    m=X.shape[0]\n",
    "    for i in range(epoch):\n",
    "        error=(X*theta.T)-y#X(97,2) theta(1,2) y(97,1) error(97,1)\n",
    "        for j in range(parameters): #theta0 theta1\n",
    "            # np.multiply(error,X[:,j]) 97*1\n",
    "            # 求出theta0+theta1\n",
    "            temp[0,j]=theta[0,j]-((alpha/m)*np.sum(np.multiply(error,X[:,j])))\n",
    "        theta=temp\n",
    "        #把每次迭代的(批量梯度下降法)更新得到的theta带入下式中,计算出J_theta\n",
    "        cost[i]=computeCost(X,y,theta)\n",
    "    return theta,cost\n",
    "alpha = 0.01\n",
    "epoch = 1000\n",
    "#得到更新的最后的theta\n",
    "final_theta, cost = gradientDescent(X, y, theta, alpha, epoch)\n",
    "#带入下式中,计算损失函数J_theta,即预测的值与实际值差多少\n",
    "J_theta=computeCost(X, y, final_theta)\n",
    "\n",
    "#现在我们绘制线性模型以及数据,可以直观的看出它的拟合\n",
    "X=np.linespace(data.Population.min(),data.Population.max(),100)\n",
    "#预测值\n",
    "f=final_theta[0,0]+(final_theta[0,1]*x) #theta0*1+theta1*x,为线性函数\n",
    "ax.plot(x,f,'r',label='prediction')\n",
    "\n",
    "ax.scatter(data['Population'],data.Profit,label='training data') #实际数据\n",
    "\n",
    "#绘制损失函数,随着迭代次数,J_theta是减小的\n",
    "ax.plot(np.arange(epoch),cost,'r')\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('cost')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多变量线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "练习1还包括一个房屋价格数据集，其中有2个变量（房子的大小，卧室的数量）和目标（房子的价格）。 我们使用我们已经应用的技术来分析数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-29ab8fe4845c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ex1data2.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'bedrooms'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'price'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;31m#预处理--特征归一化\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata2\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdata2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdata2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;31m#X [98,3]  X0=1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "path='ex1data2.txt'\n",
    "data2=pd.read_csv(path,header=None,names=['size','bedrooms','price'])\n",
    "#预处理--特征归一化\n",
    "data2=(data2-data2.mean())/data2.std()\n",
    "#X [98,3]  X0=1\n",
    "#theta0,theta1,theta2\n",
    "#与单变量类似，只不过x多出来了一列\n",
    "# add ones column\n",
    "data2.insert(0, 'Ones', 1)\n",
    "\n",
    "# set X (training data) and y (target variable)\n",
    "cols = data2.shape[1]\n",
    "X2 = data2.iloc[:,0:cols-1]\n",
    "y2 = data2.iloc[:,cols-1:cols]\n",
    "\n",
    "# convert to matrices and initialize theta\n",
    "X2 = np.matrix(X2.values)\n",
    "y2 = np.matrix(y2.values)\n",
    "theta2 = np.matrix(np.array([0,0,0]))\n",
    "\n",
    "# perform linear regression on the data set\n",
    "g2, cost2 = gradientDescent(X2, y2, theta2, alpha, epoch)\n",
    "\n",
    "# get the cost (error) of the model\n",
    "computeCost(X2, y2, g2), g2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(np.arange(epoch), cost2, 'r')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_title('Error vs. Training Epoch')\n",
    "plt.show()\n",
    "                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以使用scikit-learn的线性回归函数，而不是从头开始实现这些算法。 我们将scikit-learn的线性回归算法应用于第1部分的数据，并看看它的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-51ff3cb43fd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#线性回归算法\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#利用梯度下降法，更新theta值，使得损失函数越来越小\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "model = linear_model.LinearRegression() #线性回归算法\n",
    "model.fit(X, y) #利用梯度下降法，更新theta值，使得损失函数越来越小\n",
    "\n",
    "x = np.array(X[:, 1].A1)\n",
    "#将最后一次更新的theta值与X相乘，得到预测值f\n",
    "f = model.predict(X).flatten()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.plot(x, f, 'r', label='Prediction')\n",
    "ax.scatter(data.Population, data.Profit, label='Traning Data')\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel('Population')\n",
    "ax.set_ylabel('Profit')\n",
    "ax.set_title('Predicted Profit vs. Population Size')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正规方程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-92835a5f0d45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0my\u001b[0m\u001b[1;31m#X.T@X等价于X.T.dot(X)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfinal_theta2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnormalEqn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#感觉和批量梯度下降的theta的值有点差距\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfinal_theta2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# 正规方程\n",
    "def normalEqn(X, y):\n",
    "    theta = np.linalg.inv(X.T@X)@X.T@y#X.T@X等价于X.T.dot(X)\n",
    "    return theta\n",
    "final_theta2=normalEqn(X, y)#感觉和批量梯度下降的theta的值有点差距\n",
    "final_theta2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
